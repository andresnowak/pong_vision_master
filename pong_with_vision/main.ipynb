{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import (\n",
    "    DummyVecEnv,\n",
    "    VecFrameStack,\n",
    "    VecTransposeImage,\n",
    ")\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "import os\n",
    "import ale_py\n",
    "import torch\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c05514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):    \n",
    "\t# Convert to grayscale    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)    \n",
    "    \n",
    "    # Resize the frame    \n",
    "    resized_frame = cv2.resize(gray_frame, (84, 84))    \n",
    "    \n",
    "    # Normalize pixel values   \n",
    "    normalized_frame = resized_frame / 255.0    \n",
    "    \n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f01ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id: str, n_stack: int, seed=None):\n",
    "    env = gym.make(env_id)\n",
    "    # Apply custom preprocessing *first*\n",
    "    # Add other wrappers if needed (e.g., Monitor for logging)\n",
    "    # Note: If using NoFrameskip, you might add MaxAndSkipEnv here\n",
    "    # env = MaxAndSkipEnv(env, skip=4) # Example if starting with NoFrameskip\n",
    "\n",
    "    # env = PongHitRewardWrapper(env)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])  # Wrap in vectorized environment\n",
    "    env = VecFrameStack(env, n_stack=n_stack)\n",
    "    env = VecTransposeImage(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016c8f0",
   "metadata": {},
   "source": [
    "**Make atari env already does some preporcessing like making the images cropped or making them grayscale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577f99b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c8dc9fa15a4c0690bc20420840bc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=40000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=40000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=80000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=80000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=120000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=120000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=160000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=160000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=200000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=200000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=240000, episode_reward=-15.00 +/- 5.80\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=240000, episode_reward=-15.00 +/- 5.80\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 5505.80 +/- 2354.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 5505.80 +/- 2354.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=280000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=280000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=320000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=320000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3123.20 +/- 54.87\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3123.20 +/- 54.87\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=360000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=360000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=400000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=400000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3152.00 +/- 117.58\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3152.00 +/- 117.58\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=440000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=440000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=480000, episode_reward=-21.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=480000, episode_reward=-21.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 3056.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 3056.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -21.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "TOTAL_TIMESTEPS = 500_000  # Total training timesteps, it seems they even use 10_000_000 steps, but they se frame skip with atariwrapper (talking about stable baselines zoo)\n",
    "EVAL_FREQ = 10_000  # Evaluate every N timesteps\n",
    "LOG_DIR = \"./logs/dqn_pong_full_vision/\"\n",
    "MODEL_SAVE_PATH = \"./models/dqn_pong_full_vision\"\n",
    "TRAIN = True  # Set to False to load and evaluate a trained model\n",
    "N_STACK = 4\n",
    "\n",
    "ENV_ID = \"PongNoFrameskip-v4\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def main():\n",
    "    gym.register_envs(ale_py)\n",
    "\n",
    "    # Create environment using RAM observations instead of pixels\n",
    "    env = make_env(ENV_ID, N_STACK)\n",
    "    env = make_atari_env(ENV_ID, n_envs=4) \n",
    "    env = VecFrameStack(env, n_stack=N_STACK)\n",
    "    env = VecTransposeImage(env)\n",
    "    \n",
    "    # Callback for evaluation during training\n",
    "    eval_env = make_env(ENV_ID, N_STACK)\n",
    "    eval_env = make_atari_env(ENV_ID, n_envs=1)\n",
    "    eval_env = VecFrameStack(eval_env, n_stack=N_STACK)\n",
    "    eval_env = VecTransposeImage(eval_env)\n",
    "    \n",
    "    # Stop training when mean reward reaches 18 (Pong is scored between -21 and +21)\n",
    "    callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=18, verbose=1)\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        callback_on_new_best=callback_on_best,\n",
    "        best_model_save_path=MODEL_SAVE_PATH,\n",
    "        log_path=LOG_DIR,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    if TRAIN:\n",
    "        # Create model with MLP policy instead of CNN\n",
    "        model = DQN(\n",
    "            \"CnnPolicy\",\n",
    "            env,\n",
    "            verbose=0,\n",
    "            buffer_size=10_000,\n",
    "            learning_starts=100_000,\n",
    "            batch_size=32,\n",
    "            gamma=0.99,\n",
    "            train_freq=4,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=1_000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.01,\n",
    "            learning_rate=1e-4,\n",
    "            seed=SEED,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            device=device,\n",
    "            # optimize_memory_usage=True,\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        model.learn(\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            callback=eval_callback,\n",
    "            progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Save the final model\n",
    "        model.save(f\"{MODEL_SAVE_PATH}_final\")\n",
    "    else:\n",
    "        # Load the trained model\n",
    "        model = DQN.load(f\"{MODEL_SAVE_PATH}_best\", env=env)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mean_reward, std_reward = evaluate_model(model, eval_env, n_eval_episodes=10)\n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    # Close environments\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "\n",
    "def evaluate_model(model, eval_env, n_eval_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate a RL model\n",
    "    :param model: (BaseAlgorithm) The RL model\n",
    "    :param eval_env: (gym.Env) The evaluation environment\n",
    "    :param n_eval_episodes: (int) Number of episodes to evaluate\n",
    "    :return: (float, float) Mean reward and standard deviation\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = eval_env.step(action)\n",
    "            total_reward += reward\n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    \n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_PATH = \"./models/dqn_pong_full_vision_final\"  # Update this to your model path\n",
    "NUM_EPISODES = 1  # Number of games to play\n",
    "RENDER_DELAY = 0.05  # Delay between frames (in seconds) for better visualization\n",
    "N_STACK = 4\n",
    "\n",
    "def load_model(env):\n",
    "    # Load the trained model\n",
    "    model = DQN.load(MODEL_PATH, env=env)\n",
    "    return model\n",
    "\n",
    "def watch_agent_play():\n",
    "    # Create environment\n",
    "    env = gym.make(\"PongDeterministic-v4\", render_mode=\"human\")\n",
    "    # env = PongHitRewardWrapper(env)\n",
    "    env = DummyVecEnv([lambda: env])  # Wrap in vectorized environment\n",
    "    env = VecFrameStack(env, n_stack=N_STACK)\n",
    "    \n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_model(env)\n",
    "    \n",
    "    for episode in range(1, NUM_EPISODES + 1):\n",
    "        # obs, _ = env.reset()\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        frames = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Show the game screen\n",
    "            env.render()\n",
    "            \n",
    "            # Get action from the model\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Take the action\n",
    "            # obs, reward, done, truncated, info = env.step(action)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # done = done or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            frames += 1\n",
    "            \n",
    "            # Add small delay to make the game watchable\n",
    "            time.sleep(RENDER_DELAY)\n",
    "        \n",
    "        print(f\"Episode {episode}: Total reward: {total_reward}, Frames: {frames}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2866328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total reward: [-15.7], Frames: 888\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "watch_agent_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bc208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_intelligence_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
